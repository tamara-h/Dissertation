\section{Data Pre-Processing}

\subsection{Tokenisiation}

Splitting the words up using a 'Bag of Words' method, where the order of the words does not matter, just the frequency of the words within the sentence.

By not taking any training into account and just

The disadvantage of this method is that it loses some of the context in which the words are written in. An example is the sentence "They were not happy". Taking each word individually, this can cause mis-evaluation, and a way to improve this is to use N-Grams to tokenise the words. 

N-Grams in a text analysis approach will be to take the set of words, so that the spatial awareness within the sentence can be taken into account. For example when the above example sentence is put into a bigram model, the tokens are as follows:

\begin{center}
"They were" \\
"were not"\\
"not happy"\\
\end{center}

Here we can see the 'not' will be taken into account, negating the sentiment of the "happy", giving a better understanding of the true sentiment.

A varying value for N will be trialled as part of the project.

\subsection{Stemming and Lemmatization}

Stemming and Lemmatizing words reduces words down to their base, for example the word "Approaching" can be reduced down to the word "Approach". The sentiment behind the word will stay the same, it just removes the affixes, allowing for easier processing. This will be done by using one of the many integrated NLTK lemmatizing libraries \cite{lemmatizing}

\subsection{Handling Negation}

The use of the word "not" before a strongly valued word needs to be taken into account as can change the whole sentiment of a sentence. 
What has been used before to great effect was to reverse the sentiment score of every word appearing between a negation and a punctuation mark (.,!?:;) \cite{kolchyna2015twitter}. 

\subsection{Stop Words}

Stop words are words which carry a connecting function in the sentence, such as prepositions and have no true influence on the emotions behind the sentence. Research has been done on the best way to remove these irrelevant words \cite{saif2014stopwords}, and it has been found that removing the most common irrelevant words compiled in a list, words such as "don't" or "and", causes classification algorithms to decrease in accuracy. Exploring this further will be a stretch goal for the project, since there are suggestions that small changes, such as removing words that only occur once in a body of text help improve prediction accuracy. 